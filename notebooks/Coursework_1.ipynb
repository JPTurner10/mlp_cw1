{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Coursework 1\n",
    "\n",
    "This notebook is intended to be used as a starting point for your experiments. The instructions can be found in the MLP2024_25_CW1_Spec.pdf (see Learn,  Assignment Submission, Coursework 1). The methods provided here are just helper functions. If you want more complex graphs such as side by side comparisons of different experiments you should learn more about matplotlib and implement them. Before each experiment remember to re-initialize neural network weights and reset the data providers so you get a properly initialized experiment. For each experiment try to keep most hyperparameters the same except the one under investigation so you can understand what the effects of each are."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "plt.style.use('ggplot')\n",
    "\n",
    "def train_model_and_plot_stats(\n",
    "        model, error, learning_rule, train_data, valid_data, num_epochs, stats_interval, notebook=True):\n",
    "    \n",
    "    # As well as monitoring the error over training also monitor classification\n",
    "    # accuracy i.e. proportion of most-probable predicted classes being equal to targets\n",
    "    data_monitors={'acc': lambda y, t: (y.argmax(-1) == t.argmax(-1)).mean()}\n",
    "\n",
    "    # Use the created objects to initialise a new Optimiser instance.\n",
    "    optimiser = Optimiser(\n",
    "        model, error, learning_rule, train_data, valid_data, data_monitors, notebook=notebook)\n",
    "\n",
    "    # Run the optimiser for num_epochs epochs (full passes through the training set)\n",
    "    # printing statistics every epoch.\n",
    "    stats, keys, run_time = optimiser.train(num_epochs=num_epochs, stats_interval=stats_interval)\n",
    "\n",
    "    # Plot the change in the validation and training set error over training.\n",
    "    #fig_1 = plt.figure(figsize=(8, 4))\n",
    "    #ax_1 = fig_1.add_subplot(111)\n",
    "    #for k in ['error(train)', 'error(valid)']:\n",
    "    #    ax_1.plot(np.arange(1, stats.shape[0]) * stats_interval, \n",
    "    #              stats[1:, keys[k]], label=k)\n",
    "    #ax_1.legend(loc=0)\n",
    "    #ax_1.set_xlabel('Epoch number')\n",
    "    #ax_1.set_ylabel('Error')\n",
    "\n",
    "    # Plot the change in the validation and training set accuracy over training.\n",
    "    #fig_2 = plt.figure(figsize=(8, 4))\n",
    "    #ax_2 = fig_2.add_subplot(111)\n",
    "    #for k in ['acc(train)', 'acc(valid)']:\n",
    "    #    ax_2.plot(np.arange(1, stats.shape[0]) * stats_interval, \n",
    "    #              stats[1:, keys[k]], label=k)\n",
    "    #ax_2.legend(loc=0)\n",
    "    #ax_2.set_xlabel('Epoch number')\n",
    "    #ax_2.set_xlabel('Accuracy')\n",
    "    \n",
    "    return stats, keys, run_time#, fig_1, ax_1, fig_2, ax_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KeysView(NpzFile 'C:\\\\Users\\\\jptur\\\\~\\\\mlpractical\\\\data\\\\emnist-train.npz' with keys: inputs, targets)\n",
      "KeysView(NpzFile 'C:\\\\Users\\\\jptur\\\\~\\\\mlpractical\\\\data\\\\emnist-valid.npz' with keys: inputs, targets)\n"
     ]
    }
   ],
   "source": [
    "# The below code will set up the data providers, random number\n",
    "# generator and logger objects needed for training runs. As\n",
    "# loading the data from file take a little while you generally\n",
    "# will probably not want to reload the data providers on\n",
    "# every training run. If you wish to reset their state you\n",
    "# should instead use the .reset() method of the data providers.\n",
    "import numpy as np\n",
    "import logging\n",
    "import sys\n",
    "# sys.path.append('/path/to/mlpractical')\n",
    "from mlp.data_providers import MNISTDataProvider, EMNISTDataProvider\n",
    "\n",
    "# Seed a random number generator\n",
    "seed = 11102019 \n",
    "rng = np.random.RandomState(seed)\n",
    "batch_size = 100\n",
    "# Set up a logger object to print info about the training run to stdout\n",
    "logger = logging.getLogger()\n",
    "logger.setLevel(logging.INFO)\n",
    "logger.handlers = [logging.StreamHandler()]\n",
    "\n",
    "# Create data provider objects for the MNIST data set\n",
    "train_data = EMNISTDataProvider('train', batch_size=batch_size, rng=rng)\n",
    "valid_data = EMNISTDataProvider('valid', batch_size=batch_size, rng=rng)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tqdm in c:\\users\\jptur\\.conda\\envs\\mlp\\lib\\site-packages (4.66.5)\n",
      "Requirement already satisfied: colorama in c:\\users\\jptur\\.conda\\envs\\mlp\\lib\\site-packages (from tqdm) (0.4.6)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "# The model set up code below is provided as a starting point.\n",
    "# You will probably want to add further code cells for the\n",
    "# different experiments you run.\n",
    "\n",
    "%pip install tqdm\n",
    "\n",
    "from mlp.layers import AffineLayer, SoftmaxLayer, SigmoidLayer, ReluLayer\n",
    "from mlp.errors import CrossEntropySoftmaxError\n",
    "from mlp.models import MultipleLayerModel\n",
    "from mlp.initialisers import ConstantInit, GlorotUniformInit\n",
    "from mlp.learning_rules import AdamLearningRule\n",
    "from mlp.optimisers import Optimiser\n",
    "\n",
    "# Setup hyperparameters\n",
    "learning_rate = 0.0009\n",
    "num_epochs = 100\n",
    "stats_interval = 1\n",
    "input_dim, output_dim, hidden_dim = 784, 47, 128\n",
    "\n",
    "weights_init = GlorotUniformInit(rng=rng)\n",
    "biases_init = ConstantInit(0.)\n",
    "\n",
    "# Create model with ONE hidden layer\n",
    "#model = MultipleLayerModel([\n",
    "#    AffineLayer(input_dim, hidden_dim, weights_init, biases_init), # hidden layer\n",
    "#    ReluLayer(),\n",
    "#    AffineLayer(hidden_dim, output_dim, weights_init, biases_init) # output layer\n",
    "#])\n",
    "\n",
    "\n",
    "error = CrossEntropySoftmaxError()\n",
    "# Use a Adam learning rule\n",
    "learning_rule = AdamLearningRule(learning_rate=learning_rate)\n",
    "\n",
    "# Remember to use notebook=False when you write a script to be run in a terminal\n",
    "#_ = train_model_and_plot_stats(\n",
    "#    model, error, learning_rule, train_data, valid_data, num_epochs, stats_interval, notebook=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.0009\n",
    "num_epochs = 100\n",
    "stats_interval = 1\n",
    "\n",
    "input_dim, output_dim = 784, 47\n",
    "\n",
    "weights_init = GlorotUniformInit(rng=rng)\n",
    "biases_init = ConstantInit(0.)\n",
    "\n",
    "error = CrossEntropySoftmaxError()\n",
    "# Use a Adam learning rule\n",
    "learning_rule = AdamLearningRule(learning_rate=learning_rate)\n",
    "\n",
    "hidden_dims_list = [32, 64, 128]\n",
    "\n",
    "#-----------------Thirty two----------------------\n",
    "\n",
    "train_data.reset()\n",
    "valid_data.reset()\n",
    "\n",
    "hidden_dim = 32\n",
    "    \n",
    "model = MultipleLayerModel([\n",
    "    AffineLayer(input_dim, hidden_dim, weights_init, biases_init), # hidden layer\n",
    "    ReluLayer(),\n",
    "    AffineLayer(hidden_dim, output_dim, weights_init, biases_init) # output layer\n",
    "])\n",
    "\n",
    "thirty_two = train_model_and_plot_stats(\n",
    "    model, error, learning_rule, train_data, valid_data, num_epochs, stats_interval, notebook=True)\n",
    "\n",
    "#-----------------Sixty four----------------------\n",
    "\n",
    "train_data.reset()\n",
    "valid_data.reset()\n",
    "\n",
    "hidden_dim = 64\n",
    "    \n",
    "model = MultipleLayerModel([\n",
    "    AffineLayer(input_dim, hidden_dim, weights_init, biases_init), # hidden layer\n",
    "    ReluLayer(),\n",
    "    AffineLayer(hidden_dim, output_dim, weights_init, biases_init) # output layer\n",
    "])\n",
    "\n",
    "sixty_four = train_model_and_plot_stats(\n",
    "    model, error, learning_rule, train_data, valid_data, num_epochs, stats_interval, notebook=True)\n",
    "\n",
    "#---------------------------128------------\n",
    "\n",
    "train_data.reset()\n",
    "valid_data.reset()\n",
    "\n",
    "hidden_dim = 128\n",
    "    \n",
    "model = MultipleLayerModel([\n",
    "    AffineLayer(input_dim, hidden_dim, weights_init, biases_init), # hidden layer\n",
    "    ReluLayer(),\n",
    "    AffineLayer(hidden_dim, output_dim, weights_init, biases_init) # output layer\n",
    "])\n",
    "\n",
    "one_twenty_eight = train_model_and_plot_stats(\n",
    "    model, error, learning_rule, train_data, valid_data, num_epochs, stats_interval, notebook=True)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(learning_rate)\n",
    "\n",
    "widths = [(thirty_two, '32'), (sixty_four, '64'), (one_twenty_eight, '128')]\n",
    "\n",
    "#return stats, keys, run_time, fig_1, ax_1, fig_2, ax_2\n",
    "\n",
    "fig_1 = plt.figure(figsize=(8, 4))\n",
    "ax_1 = fig_1.add_subplot(111)\n",
    "for width in widths:\n",
    "    ax_1.plot(np.arange(1, width[0][0].shape[0]) * stats_interval, \n",
    "          width[0][0][1:, width[0][1]['error(train)']], label='width '+ width[1]+'(train)')\n",
    "for width in widths:\n",
    "    ax_1.plot(np.arange(1, width[0][0].shape[0]) * stats_interval, \n",
    "          width[0][0][1:, width[0][1]['error(valid)']], label='width '+ width[1]+'(valid)', linestyle='dashed')\n",
    "ax_1.legend(loc=0)\n",
    "ax_1.set_xlabel('Epoch number')\n",
    "ax_1.set_ylabel('Error')\n",
    "\n",
    "plt.savefig('error_curve_width.png')\n",
    "\n",
    "    # Plot the change in the validation and training set accuracy over training.\n",
    "fig_2 = plt.figure(figsize=(8, 4))\n",
    "ax_2 = fig_2.add_subplot(111)\n",
    "for width in widths:\n",
    "    ax_2.plot(np.arange(1, width[0][0].shape[0]) * stats_interval, \n",
    "          width[0][0][1:, width[0][1]['acc(train)']], label='width '+ width[1]+'(train)')\n",
    "for width in widths:\n",
    "    ax_2.plot(np.arange(1, width[0][0].shape[0]) * stats_interval, \n",
    "          width[0][0][1:, width[0][1]['acc(valid)']], label='width '+ width[1]+'(valid)', linestyle='dashed')\n",
    "ax_2.legend(loc=0)\n",
    "ax_2.set_xlabel('Epoch number')\n",
    "ax_2.set_xlabel('Accuracy')\n",
    "\n",
    "plt.savefig('acc_curve_width.png')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_dim = 128\n",
    "train_data.reset()\n",
    "valid_data.reset()\n",
    "\n",
    "# Create model with TWO hidden layers\n",
    "model = MultipleLayerModel([\n",
    "    AffineLayer(input_dim, hidden_dim, weights_init, biases_init), # first hidden layer\n",
    "    ReluLayer(),\n",
    "    AffineLayer(hidden_dim, hidden_dim, weights_init, biases_init), # second hidden layer\n",
    "    ReluLayer(),\n",
    "    AffineLayer(hidden_dim, output_dim, weights_init, biases_init) # output layer\n",
    "])\n",
    "\n",
    "# Remember to use notebook=False when you write a script to be run in a terminal\n",
    "two_layer = train_model_and_plot_stats(\n",
    "    model, error, learning_rule, train_data, valid_data, num_epochs, stats_interval, notebook=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create model with ONE hidden layers\n",
    "train_data.reset()\n",
    "valid_data.reset()\n",
    "\n",
    "model = MultipleLayerModel([\n",
    "    AffineLayer(input_dim, hidden_dim, weights_init, biases_init), # first hidden layer\n",
    "    ReluLayer(),\n",
    "    AffineLayer(hidden_dim, output_dim, weights_init, biases_init) # output layer\n",
    "])\n",
    "\n",
    "# Remember to use notebook=False when you write a script to be run in a terminal\n",
    "one_layer = train_model_and_plot_stats(\n",
    "    model, error, learning_rule, train_data, valid_data, num_epochs, stats_interval, notebook=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create model with THREE hidden layers\n",
    "train_data.reset()\n",
    "valid_data.reset()\n",
    "\n",
    "model = MultipleLayerModel([\n",
    "    AffineLayer(input_dim, hidden_dim, weights_init, biases_init), # first hidden layer\n",
    "    ReluLayer(),\n",
    "    AffineLayer(hidden_dim, hidden_dim, weights_init, biases_init), # second hidden layer\n",
    "    ReluLayer(),\n",
    "    AffineLayer(hidden_dim, hidden_dim, weights_init, biases_init), # third hidden layer\n",
    "    ReluLayer(),\n",
    "    AffineLayer(hidden_dim, output_dim, weights_init, biases_init) # output layer\n",
    "])\n",
    "\n",
    "# Remember to use notebook=False when you write a script to be run in a terminal\n",
    "three_layer = train_model_and_plot_stats(\n",
    "    model, error, learning_rule, train_data, valid_data, num_epochs, stats_interval, notebook=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "depths = [(one_layer, '1'), (two_layer, '2'), (three_layer, '3')]\n",
    "\n",
    "#return stats, keys, run_time, fig_1, ax_1, fig_2, ax_2\n",
    "\n",
    "fig_1 = plt.figure(figsize=(8, 4))\n",
    "ax_1 = fig_1.add_subplot(111)\n",
    "for depth in depths:\n",
    "    ax_1.plot(np.arange(1, depth[0][0].shape[0]) * stats_interval, \n",
    "          depth[0][0][1:, depth[0][1]['error(train)']], label='depth '+ depth[1]+'(train)')\n",
    "for depth in depths:\n",
    "    ax_1.plot(np.arange(1, depth[0][0].shape[0]) * stats_interval, \n",
    "          depth[0][0][1:, depth[0][1]['error(valid)']], label='depth '+ depth[1]+'(valid)', linestyle='dashed')\n",
    "ax_1.legend(loc=0)\n",
    "ax_1.set_xlabel('Epoch number')\n",
    "ax_1.set_ylabel('Error')\n",
    "\n",
    "plt.savefig('error_curve_depth.png')\n",
    "\n",
    "    # Plot the change in the validation and training set accuracy over training.\n",
    "fig_2 = plt.figure(figsize=(8, 4))\n",
    "ax_2 = fig_2.add_subplot(111)\n",
    "for depth in depths:\n",
    "    ax_2.plot(np.arange(1, depth[0][0].shape[0]) * stats_interval, \n",
    "          depth[0][0][1:, depth[0][1]['acc(train)']], label='depth '+ depth[1]+'(train)')\n",
    "for depth in depths:\n",
    "    ax_2.plot(np.arange(1, depth[0][0].shape[0]) * stats_interval, \n",
    "          depth[0][0][1:, depth[0][1]['acc(valid)']], label='depth '+ depth[1]+'(valid)', linestyle='dashed')\n",
    "ax_2.legend(loc=0)\n",
    "ax_2.set_xlabel('Epoch number')\n",
    "ax_2.set_xlabel('Accuracy')\n",
    "\n",
    "plt.savefig('acc_curve_depth.png')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('----Changing amount of ReLu units----')\n",
    "print('keys: ' + str(depths[0][0][1]))\n",
    "for width in widths:\n",
    "    print(str(width[1]) + ': ' + str(width[0][0][-1]))\n",
    "print('-----Changing amount of ReLu Layers----')\n",
    "for depth in depths:\n",
    "    print(str(depth[1]) + ': ' + str(depth[0][0][-1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0001\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'CrossEntropySoftmaxError' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 10\u001b[0m\n\u001b[0;32m      7\u001b[0m stats_interval \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m      8\u001b[0m input_dim, output_dim, hidden_dim \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m784\u001b[39m, \u001b[38;5;241m47\u001b[39m, \u001b[38;5;241m128\u001b[39m\n\u001b[1;32m---> 10\u001b[0m error \u001b[38;5;241m=\u001b[39m CrossEntropySoftmaxError()\n\u001b[0;32m     11\u001b[0m \u001b[38;5;66;03m# Use a Adam learning rule\u001b[39;00m\n\u001b[0;32m     12\u001b[0m learning_rule \u001b[38;5;241m=\u001b[39m AdamLearningRule(learning_rate\u001b[38;5;241m=\u001b[39mlearning_rate)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'CrossEntropySoftmaxError' is not defined"
     ]
    }
   ],
   "source": [
    "train_data.reset()\n",
    "valid_data.reset()\n",
    "\n",
    "learning_rate = 10 ** -4\n",
    "print(learning_rate)\n",
    "num_epochs = 100\n",
    "stats_interval = 1\n",
    "input_dim, output_dim, hidden_dim = 784, 47, 128\n",
    "\n",
    "error = CrossEntropySoftmaxError()\n",
    "# Use a Adam learning rule\n",
    "learning_rule = AdamLearningRule(learning_rate=learning_rate)\n",
    "\n",
    "weights_init = GlorotUniformInit(rng=rng)\n",
    "biases_init = ConstantInit(0.)\n",
    "\n",
    "\n",
    "model = MultipleLayerModel([\n",
    "    AffineLayer(input_dim, hidden_dim, weights_init, biases_init), # first hidden layer\n",
    "    ReluLayer(),\n",
    "    AffineLayer(hidden_dim, hidden_dim, weights_init, biases_init), # second hidden layer\n",
    "    ReluLayer(),\n",
    "    AffineLayer(hidden_dim, hidden_dim, weights_init, biases_init), # third hidden layer\n",
    "    ReluLayer(),\n",
    "    AffineLayer(hidden_dim, output_dim, weights_init, biases_init) # output layer\n",
    "])\n",
    "\n",
    "# Remember to use notebook=False when you write a script to be run in a terminal\n",
    "baseline = train_model_and_plot_stats(\n",
    "    model, error, learning_rule, train_data, valid_data, num_epochs, stats_interval, notebook=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('keys: ' + str(baseline[0][1]))\n",
    "print(str(baseline[0][-1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mlp.layers import DropoutLayer\n",
    "\n",
    "#----------DROPOUT--------------\n",
    "train_data.reset()\n",
    "valid_data.reset()\n",
    "\n",
    "learning_rate = 10 ** -4\n",
    "num_epochs = 100\n",
    "stats_interval = 1\n",
    "input_dim, output_dim, hidden_dim = 784, 47, 128\n",
    "\n",
    "error = CrossEntropySoftmaxError()\n",
    "# Use a Adam learning rule\n",
    "learning_rule = AdamLearningRule(learning_rate=learning_rate)\n",
    "\n",
    "weights_init = GlorotUniformInit(rng=rng)\n",
    "biases_init = ConstantInit(0.)\n",
    "\n",
    "\n",
    "model = MultipleLayerModel([\n",
    "    AffineLayer(input_dim, hidden_dim, weights_init, biases_init), # first hidden layer\n",
    "    DropoutLayer(incl_prob=0.6)\n",
    "    ReluLayer(),\n",
    "    AffineLayer(hidden_dim, hidden_dim, weights_init, biases_init), # second hidden layer\n",
    "    DropoutLayer(incl_prob=0.6)\n",
    "    ReluLayer(),\n",
    "    AffineLayer(hidden_dim, hidden_dim, weights_init, biases_init), # third hidden layer\n",
    "    DropoutLayer(incl_prob=0.6)\n",
    "    ReluLayer(),\n",
    "    AffineLayer(hidden_dim, output_dim, weights_init, biases_init) # output layer\n",
    "])\n",
    "\n",
    "# Remember to use notebook=False when you write a script to be run in a terminal\n",
    "dropout_model = train_model_and_plot_stats(\n",
    "    model, error, learning_rule, train_data, valid_data, num_epochs, stats_interval, notebook=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('keys: ' + str(dropout_model[0][1]))\n",
    "print(str(dropout_model[0][-1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mlp.penalties import L1Penalty\n",
    "\n",
    "#----------L1 Penalty--------------\n",
    "train_data.reset()\n",
    "valid_data.reset()\n",
    "\n",
    "learning_rate = 10 ** -4\n",
    "num_epochs = 100\n",
    "stats_interval = 1\n",
    "input_dim, output_dim, hidden_dim = 784, 47, 128\n",
    "\n",
    "error = CrossEntropySoftmaxError()\n",
    "# Use a Adam learning rule\n",
    "learning_rule = AdamLearningRule(learning_rate=learning_rate)\n",
    "\n",
    "weights_init = GlorotUniformInit(rng=rng)\n",
    "biases_init = ConstantInit(0.)\n",
    "\n",
    "\n",
    "model = MultipleLayerModel([\n",
    "    AffineLayer(input_dim, hidden_dim, weights_init, biases_init), # first hidden layer\n",
    "    L1Penalty(5e-4),\n",
    "    ReluLayer(),\n",
    "    AffineLayer(hidden_dim, hidden_dim, weights_init, biases_init), # second hidden layer\n",
    "    L1Penalty(5e-4),\n",
    "    ReluLayer(),\n",
    "    AffineLayer(hidden_dim, hidden_dim, weights_init, biases_init), # third hidden layer\n",
    "    L1Penalty(5e-4),\n",
    "    ReluLayer(),\n",
    "    AffineLayer(hidden_dim, output_dim, weights_init, biases_init) # output layer\n",
    "])\n",
    "\n",
    "# Remember to use notebook=False when you write a script to be run in a terminal\n",
    "l_one_model = train_model_and_plot_stats(\n",
    "    model, error, learning_rule, train_data, valid_data, num_epochs, stats_interval, notebook=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('keys: ' + str(l_one_model[0][1]))\n",
    "print(str(l_one_model[0][-1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mlp.penalties import L1Penalty\n",
    "\n",
    "#----------L2 Penalty--------------\n",
    "train_data.reset()\n",
    "valid_data.reset()\n",
    "\n",
    "learning_rate = 10 ** -4\n",
    "num_epochs = 100\n",
    "stats_interval = 1\n",
    "input_dim, output_dim, hidden_dim = 784, 47, 128\n",
    "\n",
    "error = CrossEntropySoftmaxError()\n",
    "# Use a Adam learning rule\n",
    "learning_rule = AdamLearningRule(learning_rate=learning_rate)\n",
    "\n",
    "weights_init = GlorotUniformInit(rng=rng)\n",
    "biases_init = ConstantInit(0.)\n",
    "\n",
    "\n",
    "model = MultipleLayerModel([\n",
    "    AffineLayer(input_dim, hidden_dim, weights_init, biases_init), # first hidden layer\n",
    "    L2Penalty(5e-4),\n",
    "    ReluLayer(),\n",
    "    AffineLayer(hidden_dim, hidden_dim, weights_init, biases_init), # second hidden layer\n",
    "    L2Penalty(5e-4),\n",
    "    ReluLayer(),\n",
    "    AffineLayer(hidden_dim, hidden_dim, weights_init, biases_init), # third hidden layer\n",
    "    L2Penalty(5e-4),\n",
    "    ReluLayer(),\n",
    "    AffineLayer(hidden_dim, output_dim, weights_init, biases_init) # output layer\n",
    "])\n",
    "\n",
    "# Remember to use notebook=False when you write a script to be run in a terminal\n",
    "l_two_model = train_model_and_plot_stats(\n",
    "    model, error, learning_rule, train_data, valid_data, num_epochs, stats_interval, notebook=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('keys: ' + str(l_two_model[0][1]))\n",
    "print(str(l_two_model[0][-1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mlp.penalties import L1Penalty\n",
    "\n",
    "#----------Label smoothing--------------\n",
    "train_data.reset()\n",
    "valid_data.reset()\n",
    "\n",
    "learning_rate = 10 ** -4\n",
    "num_epochs = 100\n",
    "stats_interval = 1\n",
    "input_dim, output_dim, hidden_dim = 784, 47, 128\n",
    "\n",
    "error = CrossEntropySoftmaxError()\n",
    "# Use a Adam learning rule\n",
    "learning_rule = AdamLearningRule(learning_rate=learning_rate)\n",
    "\n",
    "weights_init = GlorotUniformInit(rng=rng)\n",
    "biases_init = ConstantInit(0.)\n",
    "\n",
    "train_data = EMNISTDataProvider('train', batch_size=batch_size, rng=rng, smooth_labels=True)\n",
    "valid_data = EMNISTDataProvider('valid', batch_size=batch_size, rng=rng, smooth_labels=True)\n",
    "\n",
    "\n",
    "model = MultipleLayerModel([\n",
    "    AffineLayer(input_dim, hidden_dim, weights_init, biases_init), # first hidden layer\n",
    "    ReluLayer(),\n",
    "    AffineLayer(hidden_dim, hidden_dim, weights_init, biases_init), # second hidden layer\n",
    "    ReluLayer(),\n",
    "    AffineLayer(hidden_dim, hidden_dim, weights_init, biases_init), # third hidden layer\n",
    "    ReluLayer(),\n",
    "    AffineLayer(hidden_dim, output_dim, weights_init, biases_init) # output layer\n",
    "])\n",
    "\n",
    "# Remember to use notebook=False when you write a script to be run in a terminal\n",
    "label_smooth_model = train_model_and_plot_stats(\n",
    "    model, error, learning_rule, train_data, valid_data, num_epochs, stats_interval, notebook=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
